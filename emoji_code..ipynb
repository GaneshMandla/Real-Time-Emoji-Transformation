{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69492917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Libraries\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7eb1044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the convolution network architecture\n",
    "face_model = Sequential()\n",
    "face_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "face_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Dropout(0.25))\n",
    "face_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Dropout(0.25))\n",
    "face_model.add(Flatten())\n",
    "face_model.add(Dense(1024, activation='relu'))\n",
    "face_model.add(Dropout(0.5))\n",
    "face_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c29fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved weights\n",
    "face_model.load_weights('recognition_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46f67a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable OpenCL\n",
    "cv2.ocl.setUseOpenCL(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36180933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets Dictionaries\n",
    "facial_dict = {0: \"   Angry   \", 1: \"Disgusted\", 2: \"  Fearful  \", 3: \"   Happy   \", 4: \"  Neutral  \", 5: \"    Sad    \", 6: \"Surprised\"}\n",
    "#emojis_dict = {0:\"train/angry\", 1:\"train/disgusted\", 2:\"train/fearful\", 3:\"train/happy\", 4:\"train/neutral\", 5:\"train/sad\", 6:\"train/surprised\"}\n",
    "emojis_dict = {\n",
    "    0: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\angry.png\",\n",
    "    1: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\disgust.png\",\n",
    "    2: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\fear.png\",\n",
    "    3: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\happy.png\",\n",
    "    4: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\neutral.png\",\n",
    "    5: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\sad.png\",\n",
    "    6: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\surprise.png\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f538c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "global last_frame1\n",
    "last_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "global cap1\n",
    "show_text=[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c214dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get face captured and recognize emotion\n",
    "def Capture_Image():\n",
    "    global cap1\n",
    "    cap1 = cv2.VideoCapture(0)\n",
    "    if not cap1.isOpened():\n",
    "        print(\"cant open the camera1\")\n",
    "    global frame_number\n",
    "    length =int(cap1.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_number+=1\n",
    "    if frame_number>=length:\n",
    "        exit()\n",
    "    cap1.set(1,frame_number)\n",
    "    flag1, frame1 = cap1.read()\n",
    "    frame1 = cv2.resize(frame1,(600,500))\n",
    "    # It will detect the face in the video and bound it with a rectangular box\n",
    "    #bound_box = cv2.CascadeClassifier('haarcascades_cuda/haarcascade_frontalface_default.xml')\n",
    "    bounding_box = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    n_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in n_faces:\n",
    "        cv2.rectangle(frame1, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_frame = gray_frame[y:y + h, x:x + w]\n",
    "        crop_img = np.expand_dims(np.expand_dims(cv2.resize(roi_frame, (48, 48)), -1), 0)\n",
    "        prediction = face_model.predict(crop_img)\n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame1, facial_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        show_text[0]=maxindex\n",
    "\n",
    "    if flag1 is None:\n",
    "        print (\"Error!\")\n",
    "\n",
    "    elif flag1:\n",
    "        global last_frame1\n",
    "        last_frame1 = frame1.copy()\n",
    "        pic = cv2.cvtColor(last_frame1, cv2.COLOR_BGR2RGB) #to store the image\n",
    "        img = Image.fromarray(pic)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        lmain.imgtk = imgtk\n",
    "        lmain.configure(image=imgtk)\n",
    "        lmain.after(10, Capture_Image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ded551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show emoji according to facial expression\n",
    "def Get_Emoji():\n",
    "    frame2 = cv2.imread(emojis_dict[show_text[0]])\n",
    "    if frame2 is None:\n",
    "        print(\"Error: Could not load emoji image.\")\n",
    "        return\n",
    "    pic2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
    "    img2 = Image.fromarray(pic2)\n",
    "    imgtk2 = ImageTk.PhotoImage(image=img2)\n",
    "    lmain2.imgtk2 = imgtk2\n",
    "    lmain3.configure(text=facial_dict[show_text[0]], font=('arial', 45, 'bold'))\n",
    "    lmain2.configure(image=imgtk2)\n",
    "    root.update()\n",
    "    lmain2.after(10, Get_Emoji)\n",
    "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "910477e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 884ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "# GUI Window to show captured image with emoji\n",
    "if __name__ == '__main__':\n",
    "    frame_number=0\n",
    "    root=tk.Tk()\n",
    "    heading = Label(root,bg='black')\n",
    "    heading.pack()\n",
    "    heading2=Label(root,text=\"Face To Emojify\",pady=20, font=('arial',45,'bold'),bg='black',fg='#CDCDCD')#to label the output\n",
    "    heading2.pack()\n",
    "    lmain = tk.Label(master=root,padx=50,bd=10)\n",
    "    lmain2 = tk.Label(master=root,bd=10)\n",
    "    lmain3=tk.Label(master=root,bd=10,fg=\"#CDCDCD\",bg='black')\n",
    "    lmain.pack(side=LEFT)\n",
    "    lmain.place(x=50,y=250)\n",
    "    lmain3.pack()\n",
    "    lmain3.place(x=960,y=250)\n",
    "    lmain2.pack(side=RIGHT)\n",
    "    lmain2.place(x=900,y=350)\n",
    "    root.title(\"Face To Emojify\")\n",
    "    root.geometry(\"1400x900+100+10\")\n",
    "    root['bg']='black'\n",
    "    exitbutton = Button(root, text='Quit',fg=\"red\",command=root.destroy,font=('arial',25,'bold')).pack(side = BOTTOM)\n",
    "    Capture_Image()\n",
    "    Get_Emoji()\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15385fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 347ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    }
   ],
   "source": [
    "# Import required Libraries\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "\n",
    "\n",
    "# Build the convolution network architecture\n",
    "face_model = Sequential()\n",
    "face_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "face_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Dropout(0.25))\n",
    "face_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Dropout(0.25))\n",
    "face_model.add(Flatten())\n",
    "face_model.add(Dense(1024, activation='relu'))\n",
    "face_model.add(Dropout(0.5))\n",
    "face_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "# Load the saved weights\n",
    "face_model.load_weights('recognition_model.h5')\n",
    "\n",
    "\n",
    "\n",
    "# Disable OpenCL\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "\n",
    "\n",
    "# Create Datasets Dictionaries\n",
    "facial_dict = {0: \"   Angry   \", 1: \"Disgusted\", 2: \"  Fearful  \", 3: \"   Happy   \", 4: \"  Neutral  \", 5: \"    Sad    \", 6: \"Surprised\"}\n",
    "#emojis_dict = {0:\"train/angry\", 1:\"train/disgusted\", 2:\"train/fearful\", 3:\"train/happy\", 4:\"train/neutral\", 5:\"train/sad\", 6:\"train/surprised\"}\n",
    "emojis_dict = {\n",
    "    0: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\angry.png\",\n",
    "    1: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\disgust.png\",\n",
    "    2: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\fear.png\",\n",
    "    3: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\happy.png\",\n",
    "    4: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\neutral.png\",\n",
    "    5: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\sad.png\",\n",
    "    6: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\surprise.png\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Global variables\n",
    "global last_frame1\n",
    "last_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "global cap1\n",
    "show_text=[0]\n",
    "\n",
    "\n",
    "\n",
    "# Function to get face captured and recognize emotion\n",
    "def Capture_Image():\n",
    "    global cap1\n",
    "    cap1 = cv2.VideoCapture(0)\n",
    "    if not cap1.isOpened():\n",
    "        print(\"cant open the camera1\")\n",
    "    global frame_number\n",
    "    length =int(cap1.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_number+=1\n",
    "    if frame_number>=length:\n",
    "        exit()\n",
    "    cap1.set(1,frame_number)\n",
    "    flag1, frame1 = cap1.read()\n",
    "    frame1 = cv2.resize(frame1,(600,500))\n",
    "    # It will detect the face in the video and bound it with a rectangular box\n",
    "    #bound_box = cv2.CascadeClassifier('haarcascades_cuda/haarcascade_frontalface_default.xml')\n",
    "    bounding_box = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    n_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in n_faces:\n",
    "        cv2.rectangle(frame1, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_frame = gray_frame[y:y + h, x:x + w]\n",
    "        crop_img = np.expand_dims(np.expand_dims(cv2.resize(roi_frame, (48, 48)), -1), 0)\n",
    "        prediction = face_model.predict(crop_img)\n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame1, facial_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        show_text[0]=maxindex\n",
    "\n",
    "    if flag1 is None:\n",
    "        print (\"Error!\")\n",
    "\n",
    "    elif flag1:\n",
    "        global last_frame1\n",
    "        last_frame1 = frame1.copy()\n",
    "        pic = cv2.cvtColor(last_frame1, cv2.COLOR_BGR2RGB) #to store the image\n",
    "        img = Image.fromarray(pic)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        lmain.imgtk = imgtk\n",
    "        lmain.configure(image=imgtk)\n",
    "        lmain.after(10, Capture_Image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        exit()\n",
    "        \n",
    "\n",
    "# Function to show emoji according to facial expression\n",
    "def Get_Emoji():\n",
    "    frame2 = cv2.imread(emojis_dict[show_text[0]])\n",
    "    if frame2 is None:\n",
    "        print(\"Error: Could not load emoji image.\")\n",
    "        return\n",
    "    pic2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
    "    img2 = Image.fromarray(pic2)\n",
    "    imgtk2 = ImageTk.PhotoImage(image=img2)\n",
    "    lmain2.imgtk2 = imgtk2\n",
    "    lmain3.configure(text=facial_dict[show_text[0]], font=('arial', 45, 'bold'))\n",
    "    lmain2.configure(image=imgtk2)\n",
    "    root.update()\n",
    "    lmain2.after(10, Get_Emoji)\n",
    "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    exit()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# GUI Window to show captured image with emoji\n",
    "if __name__ == '__main__':\n",
    "    frame_number=0\n",
    "    root=tk.Tk()\n",
    "    heading = Label(root,bg='black')\n",
    "    heading.pack()\n",
    "    heading2=Label(root,text=\"Face To Emojify\",pady=20, font=('arial',45,'bold'),bg='black',fg='#CDCDCD')#to label the output\n",
    "    heading2.pack()\n",
    "    lmain = tk.Label(master=root,padx=50,bd=10)\n",
    "    lmain2 = tk.Label(master=root,bd=10)\n",
    "    lmain3=tk.Label(master=root,bd=10,fg=\"#CDCDCD\",bg='black')\n",
    "    lmain.pack(side=LEFT)\n",
    "    lmain.place(x=50,y=250)\n",
    "    lmain3.pack()\n",
    "    lmain3.place(x=960,y=250)\n",
    "    lmain2.pack(side=RIGHT)\n",
    "    lmain2.place(x=900,y=250)\n",
    "    root.title(\"Face To Emojify\")\n",
    "    root.geometry(\"1400x900+100+10\")\n",
    "    root['bg']='black'\n",
    "    exitbutton = Button(root, text='Quit',fg=\"red\",command=root.destroy,font=('arial',25,'bold')).pack(side = BOTTOM)\n",
    "    Capture_Image()\n",
    "    Get_Emoji()\n",
    "    root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437689e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "# Import required Libraries\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "\n",
    "\n",
    "# Build the convolution network architecture\n",
    "face_model = Sequential()\n",
    "face_model.add(Conv2D(32, kernel_size=(3, 3), activation='softmax', input_shape=(48,48,1)))\n",
    "face_model.add(Conv2D(64, kernel_size=(3, 3), activation='softmax'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Dropout(0.25))\n",
    "face_model.add(Conv2D(128, kernel_size=(3, 3), activation='softmax'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Conv2D(128, kernel_size=(3, 3), activation='softmax'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Dropout(0.25))\n",
    "face_model.add(Flatten())\n",
    "face_model.add(Dense(1024, activation='softmax'))\n",
    "face_model.add(Dropout(0.5))\n",
    "face_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "# Load the saved weights\n",
    "face_model.load_weights('recognition_model.h5')\n",
    "\n",
    "\n",
    "\n",
    "# Disable OpenCL\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "\n",
    "\n",
    "# Create Datasets Dictionaries\n",
    "facial_dict = {0: \"   Angry   \", 1: \"Disgusted\", 2: \"  Fearful  \", 3: \"   Happy   \", 4: \"  Neutral  \", 5: \"    Sad    \", 6: \"Surprised\"}\n",
    "#emojis_dict = {0:\"train/angry\", 1:\"train/disgusted\", 2:\"train/fearful\", 3:\"train/happy\", 4:\"train/neutral\", 5:\"train/sad\", 6:\"train/surprised\"}\n",
    "emojis_dict = {\n",
    "    0: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\angry.png\",\n",
    "    1: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\disgust.png\",\n",
    "    2: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\fear.png\",\n",
    "    3: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\happy.png\",\n",
    "    4: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\neutral.png\",\n",
    "    5: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\sad.png\",\n",
    "    6: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\surprise.png\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Global variables\n",
    "global last_frame1\n",
    "last_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "global cap1\n",
    "show_text=[0]\n",
    "\n",
    "\n",
    "\n",
    "# Function to get face captured and recognize emotion\n",
    "def Capture_Image():\n",
    "    global cap1\n",
    "    cap1 = cv2.VideoCapture(0)\n",
    "    if not cap1.isOpened():\n",
    "        print(\"cant open the camera1\")\n",
    "    global frame_number\n",
    "    length =int(cap1.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_number+=1\n",
    "    if frame_number>=length:\n",
    "        exit()\n",
    "    cap1.set(1,frame_number)\n",
    "    flag1, frame1 = cap1.read()\n",
    "    frame1 = cv2.resize(frame1,(600,500))\n",
    "    # It will detect the face in the video and bound it with a rectangular box\n",
    "    #bound_box = cv2.CascadeClassifier('haarcascades_cuda/haarcascade_frontalface_default.xml')\n",
    "    bounding_box = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    n_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in n_faces:\n",
    "        cv2.rectangle(frame1, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_frame = gray_frame[y:y + h, x:x + w]\n",
    "        crop_img = np.expand_dims(np.expand_dims(cv2.resize(roi_frame, (48, 48)), -1), 0)\n",
    "        prediction = face_model.predict(crop_img)\n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame1, facial_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        show_text[0]=maxindex\n",
    "\n",
    "    if flag1 is None:\n",
    "        print (\"Error!\")\n",
    "\n",
    "    elif flag1:\n",
    "        global last_frame1\n",
    "        last_frame1 = frame1.copy()\n",
    "        pic = cv2.cvtColor(last_frame1, cv2.COLOR_BGR2RGB) #to store the image\n",
    "        img = Image.fromarray(pic)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        lmain.imgtk = imgtk\n",
    "        lmain.configure(image=imgtk)\n",
    "        lmain.after(10, Capture_Image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        exit()\n",
    "        \n",
    "\n",
    "# Function to show emoji according to facial expression\n",
    "def Get_Emoji():\n",
    "    frame2 = cv2.imread(emojis_dict[show_text[0]])\n",
    "    if frame2 is None:\n",
    "        print(\"Error: Could not load emoji image.\")\n",
    "        return\n",
    "    pic2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
    "    img2 = Image.fromarray(pic2)\n",
    "    imgtk2 = ImageTk.PhotoImage(image=img2)\n",
    "    lmain2.imgtk2 = imgtk2\n",
    "    lmain3.configure(text=facial_dict[show_text[0]], font=('arial', 45, 'bold'))\n",
    "    lmain2.configure(image=imgtk2)\n",
    "    root.update()\n",
    "    lmain2.after(10, Get_Emoji)\n",
    "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    exit()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# GUI Window to show captured image with emoji\n",
    "if __name__ == '__main__':\n",
    "    frame_number=0\n",
    "    root=tk.Tk()\n",
    "    heading = Label(root,bg='black')\n",
    "    heading.pack()\n",
    "    heading2=Label(root,text=\"Face To Emojify\",pady=20, font=('arial',45,'bold'),bg='black',fg='#CDCDCD')#to label the output\n",
    "    heading2.pack()\n",
    "    lmain = tk.Label(master=root,padx=50,bd=10)\n",
    "    lmain2 = tk.Label(master=root,bd=10)\n",
    "    lmain3=tk.Label(master=root,bd=10,fg=\"#CDCDCD\",bg='black')\n",
    "    lmain.pack(side=LEFT)\n",
    "    lmain.place(x=50,y=250)\n",
    "    lmain3.pack()\n",
    "    lmain3.place(x=960,y=250)\n",
    "    lmain2.pack(side=RIGHT)\n",
    "    lmain2.place(x=900,y=250)\n",
    "    root.title(\"Face To Emojify\")\n",
    "    root.geometry(\"1400x900+100+10\")\n",
    "    root['bg']='black'\n",
    "    exitbutton = Button(root, text='Quit',fg=\"red\",command=root.destroy,font=('arial',25,'bold')).pack(side = BOTTOM)\n",
    "    Capture_Image()\n",
    "    Get_Emoji()\n",
    "    root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb36300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "# Import required Libraries\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "\n",
    "\n",
    "# Build the convolution network architecture\n",
    "face_model = Sequential()\n",
    "face_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "face_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Dropout(0.25))\n",
    "face_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Dropout(0.25))\n",
    "face_model.add(Flatten())\n",
    "face_model.add(Dense(1024, activation='relu'))\n",
    "face_model.add(Dropout(0.5))\n",
    "face_model.add(Dense(7, activation='relu'))\n",
    "\n",
    "\n",
    "# Load the saved weights\n",
    "face_model.load_weights('recognition_model.h5')\n",
    "\n",
    "\n",
    "\n",
    "# Disable OpenCL\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "\n",
    "\n",
    "# Create Datasets Dictionaries\n",
    "facial_dict = {0: \"   Angry   \", 1: \"Disgusted\", 2: \"  Fearful  \", 3: \"   Happy   \", 4: \"  Neutral  \", 5: \"    Sad    \", 6: \"Surprised\"}\n",
    "#emojis_dict = {0:\"train/angry\", 1:\"train/disgusted\", 2:\"train/fearful\", 3:\"train/happy\", 4:\"train/neutral\", 5:\"train/sad\", 6:\"train/surprised\"}\n",
    "emojis_dict = {\n",
    "    0: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\angry.png\",\n",
    "    1: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\disgust.png\",\n",
    "    2: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\fear.png\",\n",
    "    3: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\happy.png\",\n",
    "    4: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\neutral.png\",\n",
    "    5: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\sad.png\",\n",
    "    6: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\surprise.png\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Global variables\n",
    "global last_frame1\n",
    "last_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "global cap1\n",
    "show_text=[0]\n",
    "\n",
    "\n",
    "\n",
    "# Function to get face captured and recognize emotion\n",
    "def Capture_Image():\n",
    "    global cap1\n",
    "    cap1 = cv2.VideoCapture(0)\n",
    "    if not cap1.isOpened():\n",
    "        print(\"cant open the camera1\")\n",
    "    global frame_number\n",
    "    length =int(cap1.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_number+=1\n",
    "    if frame_number>=length:\n",
    "        exit()\n",
    "    cap1.set(1,frame_number)\n",
    "    flag1, frame1 = cap1.read()\n",
    "    frame1 = cv2.resize(frame1,(600,500))\n",
    "    # It will detect the face in the video and bound it with a rectangular box\n",
    "    #bound_box = cv2.CascadeClassifier('haarcascades_cuda/haarcascade_frontalface_default.xml')\n",
    "    bounding_box = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    n_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in n_faces:\n",
    "        cv2.rectangle(frame1, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_frame = gray_frame[y:y + h, x:x + w]\n",
    "        crop_img = np.expand_dims(np.expand_dims(cv2.resize(roi_frame, (48, 48)), -1), 0)\n",
    "        prediction = face_model.predict(crop_img)\n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame1, facial_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        show_text[0]=maxindex\n",
    "\n",
    "    if flag1 is None:\n",
    "        print (\"Error!\")\n",
    "\n",
    "    elif flag1:\n",
    "        global last_frame1\n",
    "        last_frame1 = frame1.copy()\n",
    "        pic = cv2.cvtColor(last_frame1, cv2.COLOR_BGR2RGB) #to store the image\n",
    "        img = Image.fromarray(pic)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        lmain.imgtk = imgtk\n",
    "        lmain.configure(image=imgtk)\n",
    "        lmain.after(10, Capture_Image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        exit()\n",
    "        \n",
    "\n",
    "# Function to show emoji according to facial expression\n",
    "def Get_Emoji():\n",
    "    frame2 = cv2.imread(emojis_dict[show_text[0]])\n",
    "    if frame2 is None:\n",
    "        print(\"Error: Could not load emoji image.\")\n",
    "        return\n",
    "    pic2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
    "    img2 = Image.fromarray(pic2)\n",
    "    imgtk2 = ImageTk.PhotoImage(image=img2)\n",
    "    lmain2.imgtk2 = imgtk2\n",
    "    lmain3.configure(text=facial_dict[show_text[0]], font=('arial', 45, 'bold'))\n",
    "    lmain2.configure(image=imgtk2)\n",
    "    root.update()\n",
    "    lmain2.after(10, Get_Emoji)\n",
    "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    exit()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# GUI Window to show captured image with emoji\n",
    "if __name__ == '__main__':\n",
    "    frame_number=0\n",
    "    root=tk.Tk()\n",
    "    heading = Label(root,bg='black')\n",
    "    heading.pack()\n",
    "    heading2=Label(root,text=\"Face To Emojify\",pady=20, font=('arial',45,'bold'),bg='black',fg='#CDCDCD')#to label the output\n",
    "    heading2.pack()\n",
    "    lmain = tk.Label(master=root,padx=50,bd=10)\n",
    "    lmain2 = tk.Label(master=root,bd=10)\n",
    "    lmain3=tk.Label(master=root,bd=10,fg=\"#CDCDCD\",bg='black')\n",
    "    lmain.pack(side=LEFT)\n",
    "    lmain.place(x=50,y=250)\n",
    "    lmain3.pack()\n",
    "    lmain3.place(x=960,y=250)\n",
    "    lmain2.pack(side=RIGHT)\n",
    "    lmain2.place(x=900,y=250)\n",
    "    root.title(\"Face To Emojify\")\n",
    "    root.geometry(\"1400x900+100+10\")\n",
    "    root['bg']='black'\n",
    "    exitbutton = Button(root, text='Quit',fg=\"red\",command=root.destroy,font=('arial',25,'bold')).pack(side = BOTTOM)\n",
    "    Capture_Image()\n",
    "    Get_Emoji()\n",
    "    root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730fded5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "# Import required Libraries\n",
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "\n",
    "\n",
    "# Build the convolution network architecture\n",
    "face_model = Sequential()\n",
    "face_model.add(Conv2D(32, kernel_size=(3, 3), activation='sigmoid', input_shape=(48,48,1)))\n",
    "face_model.add(Conv2D(64, kernel_size=(3, 3), activation='sigmoid'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Dropout(0.25))\n",
    "face_model.add(Conv2D(128, kernel_size=(3, 3), activation='sigmoid'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Conv2D(128, kernel_size=(3, 3), activation='sigmoid'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Dropout(0.25))\n",
    "face_model.add(Flatten())\n",
    "face_model.add(Dense(1024, activation='sigmoid'))\n",
    "face_model.add(Dropout(0.5))\n",
    "face_model.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# Load the saved weights\n",
    "face_model.load_weights('recognition_model.h5')\n",
    "\n",
    "\n",
    "\n",
    "# Disable OpenCL\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "\n",
    "\n",
    "# Create Datasets Dictionaries\n",
    "facial_dict = {0: \"   Angry   \", 1: \"Disgusted\", 2: \"  Fearful  \", 3: \"   Happy   \", 4: \"  Neutral  \", 5: \"    Sad    \", 6: \"Surprised\"}\n",
    "#emojis_dict = {0:\"train/angry\", 1:\"train/disgusted\", 2:\"train/fearful\", 3:\"train/happy\", 4:\"train/neutral\", 5:\"train/sad\", 6:\"train/surprised\"}\n",
    "emojis_dict = {\n",
    "    0: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\angry.png\",\n",
    "    1: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\disgust.png\",\n",
    "    2: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\fear.png\",\n",
    "    3: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\happy.png\",\n",
    "    4: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\neutral.png\",\n",
    "    5: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\sad.png\",\n",
    "    6: r\"C:\\Users\\mandl\\OneDrive\\Documents\\PROJECT\\MINI PROJECT-2\\emojis\\surprise.png\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Global variables\n",
    "global last_frame1\n",
    "last_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "global cap1\n",
    "show_text=[0]\n",
    "\n",
    "\n",
    "\n",
    "# Function to get face captured and recognize emotion\n",
    "def Capture_Image():\n",
    "    global cap1\n",
    "    cap1 = cv2.VideoCapture(0)\n",
    "    if not cap1.isOpened():\n",
    "        print(\"cant open the camera1\")\n",
    "    global frame_number\n",
    "    length =int(cap1.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_number+=1\n",
    "    if frame_number>=length:\n",
    "        exit()\n",
    "    cap1.set(1,frame_number)\n",
    "    flag1, frame1 = cap1.read()\n",
    "    frame1 = cv2.resize(frame1,(600,500))\n",
    "    # It will detect the face in the video and bound it with a rectangular box\n",
    "    #bound_box = cv2.CascadeClassifier('haarcascades_cuda/haarcascade_frontalface_default.xml')\n",
    "    bounding_box = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    n_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in n_faces:\n",
    "        cv2.rectangle(frame1, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_frame = gray_frame[y:y + h, x:x + w]\n",
    "        crop_img = np.expand_dims(np.expand_dims(cv2.resize(roi_frame, (48, 48)), -1), 0)\n",
    "        prediction = face_model.predict(crop_img)\n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame1, facial_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        show_text[0]=maxindex\n",
    "\n",
    "    if flag1 is None:\n",
    "        print (\"Error!\")\n",
    "\n",
    "    elif flag1:\n",
    "        global last_frame1\n",
    "        last_frame1 = frame1.copy()\n",
    "        pic = cv2.cvtColor(last_frame1, cv2.COLOR_BGR2RGB) #to store the image\n",
    "        img = Image.fromarray(pic)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        lmain.imgtk = imgtk\n",
    "        lmain.configure(image=imgtk)\n",
    "        lmain.after(10, Capture_Image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        exit()\n",
    "        \n",
    "\n",
    "# Function to show emoji according to facial expression\n",
    "def Get_Emoji():\n",
    "    frame2 = cv2.imread(emojis_dict[show_text[0]])\n",
    "    if frame2 is None:\n",
    "        print(\"Error: Could not load emoji image.\")\n",
    "        return\n",
    "    pic2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
    "    img2 = Image.fromarray(pic2)\n",
    "    imgtk2 = ImageTk.PhotoImage(image=img2)\n",
    "    lmain2.imgtk2 = imgtk2\n",
    "    lmain3.configure(text=facial_dict[show_text[0]], font=('arial', 45, 'bold'))\n",
    "    lmain2.configure(image=imgtk2)\n",
    "    root.update()\n",
    "    lmain2.after(10, Get_Emoji)\n",
    "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    exit()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# GUI Window to show captured image with emoji\n",
    "if __name__ == '__main__':\n",
    "    frame_number=0\n",
    "    root=tk.Tk()\n",
    "    heading = Label(root,bg='black')\n",
    "    heading.pack()\n",
    "    heading2=Label(root,text=\"Face To Emojify\",pady=20, font=('arial',45,'bold'),bg='black',fg='#CDCDCD')#to label the output\n",
    "    heading2.pack()\n",
    "    lmain = tk.Label(master=root,padx=50,bd=10)\n",
    "    lmain2 = tk.Label(master=root,bd=10)\n",
    "    lmain3=tk.Label(master=root,bd=10,fg=\"#CDCDCD\",bg='black')\n",
    "    lmain.pack(side=LEFT)\n",
    "    lmain.place(x=50,y=250)\n",
    "    lmain3.pack()\n",
    "    lmain3.place(x=960,y=250)\n",
    "    lmain2.pack(side=RIGHT)\n",
    "    lmain2.place(x=900,y=250)\n",
    "    root.title(\"Face To Emojify\")\n",
    "    root.geometry(\"1400x900+100+10\")\n",
    "    root['bg']='black'\n",
    "    exitbutton = Button(root, text='Quit',fg=\"red\",command=root.destroy,font=('arial',25,'bold')).pack(side = BOTTOM)\n",
    "    Capture_Image()\n",
    "    Get_Emoji()\n",
    "    root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac125226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098612a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a6746e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
